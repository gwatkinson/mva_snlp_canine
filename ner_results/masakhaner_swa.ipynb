{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"I9jCjsexmnU2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683645822573,"user_tz":-120,"elapsed":5,"user":{"displayName":"Javier Ramos","userId":"00153831238828418262"}},"outputId":"b03851be-1f73-4ece-bcbb-47f4252e332b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/NLP\n"]}],"source":["#Upload the folder manually\n","%cd /content/NLP"]},{"cell_type":"code","source":["%pip install transformers datasets seqeval"],"metadata":{"id":"4ZKQoB8XnklA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683645845200,"user_tz":-120,"elapsed":22376,"user":{"displayName":"Javier Ramos","userId":"00153831238828418262"}},"outputId":"e5b37714-bf93-4d61-d318-05726b180368"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=f030b1ee242011f1be3823daad94be0d3d6a794af68ffdfe25c4df12e09edaed\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: tokenizers, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, seqeval, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 seqeval-1.2.2 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["!python3 mbert.py --lang=swa"],"metadata":{"id":"LOAXh9OInn20","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683645992531,"user_tz":-120,"elapsed":147337,"user":{"displayName":"Javier Ramos","userId":"00153831238828418262"}},"outputId":"c0515eff-077d-416f-d9cd-69f1aa135331"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-09 15:24:13.278563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Downloading builder script: 100% 7.60k/7.60k [00:00<00:00, 6.13MB/s]\n","Downloading metadata: 100% 39.4k/39.4k [00:00<00:00, 3.81MB/s]\n","Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 10.8MB/s]\n","Downloading and preparing dataset masakhaner/swa to /root/.cache/huggingface/datasets/masakhaner/swa/1.0.0/e61b24903076a3af7682855beebb820ec64edad0d6787b148c473694592d10b3...\n","Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n","Downloading data: 491kB [00:00, 23.3MB/s]       \n","Downloading data files:  33% 1/3 [00:00<00:01,  1.19it/s]\n","Downloading data: 63.1kB [00:00, 20.8MB/s]       \n","Downloading data files:  67% 2/3 [00:01<00:00,  1.41it/s]\n","Downloading data: 132kB [00:00, 26.5MB/s]        \n","Downloading data files: 100% 3/3 [00:02<00:00,  1.45it/s]\n","Extracting data files: 100% 3/3 [00:00<00:00, 1242.76it/s]\n","Dataset masakhaner downloaded and prepared to /root/.cache/huggingface/datasets/masakhaner/swa/1.0.0/e61b24903076a3af7682855beebb820ec64edad0d6787b148c473694592d10b3. Subsequent calls will reuse this data.\n","100% 3/3 [00:00<00:00, 313.98it/s]\n","['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE']\n","Downloading (â€¦)okenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 156kB/s]\n","Downloading (â€¦)lve/main/config.json: 100% 625/625 [00:00<00:00, 2.93MB/s]\n","Downloading (â€¦)solve/main/vocab.txt: 100% 996k/996k [00:00<00:00, 11.6MB/s]\n","Downloading (â€¦)/main/tokenizer.json: 100% 1.96M/1.96M [00:00<00:00, 27.0MB/s]\n","Downloading pytorch_model.bin: 100% 714M/714M [00:13<00:00, 52.4MB/s]\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/content/NLP/mbert.py:76: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"seqeval\")\n","Downloading builder script: 6.33kB [00:00, 4.18MB/s]       \n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","  0% 0/264 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"," 50% 132/264 [00:40<00:39,  3.30it/s]\n","  0% 0/19 [00:00<?, ?it/s]\u001b[A\n"," 11% 2/19 [00:00<00:00, 19.64it/s]\u001b[A\n"," 21% 4/19 [00:00<00:00, 17.07it/s]\u001b[A\n"," 32% 6/19 [00:00<00:00, 17.59it/s]\u001b[A\n"," 42% 8/19 [00:00<00:00, 16.19it/s]\u001b[A\n"," 53% 10/19 [00:00<00:00, 16.44it/s]\u001b[A\n"," 63% 12/19 [00:00<00:00, 15.88it/s]\u001b[A\n"," 74% 14/19 [00:00<00:00, 12.32it/s]\u001b[A\n"," 84% 16/19 [00:01<00:00, 11.05it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.11269684135913849, 'eval_precision': 0.8256624825662483, 'eval_recall': 0.8783382789317508, 'eval_f1': 0.8511861969805896, 'eval_accuracy': 0.9702602230483272, 'eval_runtime': 1.6851, 'eval_samples_per_second': 178.027, 'eval_steps_per_second': 11.275, 'epoch': 1.0}\n"," 50% 132/264 [00:42<00:39,  3.30it/s]\n","100% 19/19 [00:01<00:00, 10.93it/s]\u001b[A\n","100% 264/264 [01:31<00:00,  3.52it/s]\n","  0% 0/19 [00:00<?, ?it/s]\u001b[A\n"," 11% 2/19 [00:00<00:00, 18.91it/s]\u001b[A\n"," 21% 4/19 [00:00<00:00, 17.21it/s]\u001b[A\n"," 32% 6/19 [00:00<00:00, 17.19it/s]\u001b[A\n"," 42% 8/19 [00:00<00:00, 15.83it/s]\u001b[A\n"," 53% 10/19 [00:00<00:00, 16.11it/s]\u001b[A\n"," 63% 12/19 [00:00<00:00, 15.55it/s]\u001b[A\n"," 74% 14/19 [00:00<00:00, 12.06it/s]\u001b[A\n"," 84% 16/19 [00:01<00:00, 10.79it/s]\u001b[A\n","                                     \n","\u001b[A{'eval_loss': 0.11052367836236954, 'eval_precision': 0.8034993270524899, 'eval_recall': 0.8857566765578635, 'eval_f1': 0.8426252646436133, 'eval_accuracy': 0.9642021203359493, 'eval_runtime': 1.7179, 'eval_samples_per_second': 174.632, 'eval_steps_per_second': 11.06, 'epoch': 2.0}\n","100% 264/264 [01:32<00:00,  3.52it/s]\n","100% 19/19 [00:01<00:00, 10.62it/s]\u001b[A\n","{'train_runtime': 102.4203, 'train_samples_per_second': 41.183, 'train_steps_per_second': 2.578, 'train_loss': 0.18180857282696347, 'epoch': 2.0}\n","100% 264/264 [01:42<00:00,  2.58it/s]\n","100% 38/38 [00:03<00:00, 10.79it/s]\n","\n","\n"," ---RESULTS FOR THE TEST DATASET:--- \n","\n","DATE : \n"," {'precision': 0.7106598984771574, 'recall': 0.8641975308641975, 'f1': 0.7799442896935933, 'number': 162} \n","\n","LOC : \n"," {'precision': 0.8643724696356275, 'recall': 0.9222462203023758, 'f1': 0.8923719958202716, 'number': 463} \n","\n","ORG : \n"," {'precision': 0.6582278481012658, 'recall': 0.7058823529411765, 'f1': 0.6812227074235808, 'number': 221} \n","\n","PER : \n"," {'precision': 0.8686567164179104, 'recall': 0.8738738738738738, 'f1': 0.87125748502994, 'number': 333} \n","\n","overall_precision : \n"," 0.8028503562945368 \n","\n","overall_recall : \n"," 0.8600508905852418 \n","\n","overall_f1 : \n"," 0.8304668304668305 \n","\n","overall_accuracy : \n"," 0.9680057109481471 \n","\n"]}]},{"cell_type":"code","source":["!python3 canine.py --lang=swa"],"metadata":{"id":"HpxYTarlnqMD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683646950561,"user_tz":-120,"elapsed":958052,"user":{"displayName":"Javier Ramos","userId":"00153831238828418262"}},"outputId":"8a270ff9-74a5-4cc9-8f56-f77542a99a17"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-09 15:26:40.142060: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Found cached dataset masakhaner (/root/.cache/huggingface/datasets/masakhaner/swa/1.0.0/e61b24903076a3af7682855beebb820ec64edad0d6787b148c473694592d10b3)\n","100% 3/3 [00:00<00:00, 692.09it/s]\n","['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE']\n","Downloading (â€¦)cial_tokens_map.json: 100% 657/657 [00:00<00:00, 3.19MB/s]\n","Downloading (â€¦)okenizer_config.json: 100% 892/892 [00:00<00:00, 3.80MB/s]\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Downloading (â€¦)lve/main/config.json: 100% 698/698 [00:00<00:00, 2.92MB/s]\n","Downloading pytorch_model.bin: 100% 529M/529M [00:02<00:00, 198MB/s]\n","Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/content/NLP/canine.py:93: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"seqeval\")\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","{'loss': 0.3843, 'learning_rate': 1.053030303030303e-05, 'epoch': 0.95}\n"," 50% 528/1056 [06:57<05:33,  1.59it/s]\n","  0% 0/75 [00:00<?, ?it/s]\u001b[A\n","  3% 2/75 [00:00<00:09,  7.69it/s]\u001b[A\n","  4% 3/75 [00:00<00:13,  5.25it/s]\u001b[A\n","  5% 4/75 [00:00<00:15,  4.57it/s]\u001b[A\n","  7% 5/75 [00:01<00:16,  4.30it/s]\u001b[A\n","  8% 6/75 [00:01<00:16,  4.16it/s]\u001b[A\n","  9% 7/75 [00:01<00:16,  4.03it/s]\u001b[A\n"," 11% 8/75 [00:01<00:17,  3.93it/s]\u001b[A\n"," 12% 9/75 [00:02<00:16,  3.90it/s]\u001b[A\n"," 13% 10/75 [00:02<00:16,  3.90it/s]\u001b[A\n"," 15% 11/75 [00:02<00:16,  3.87it/s]\u001b[A\n"," 16% 12/75 [00:02<00:16,  3.83it/s]\u001b[A\n"," 17% 13/75 [00:03<00:16,  3.83it/s]\u001b[A\n"," 19% 14/75 [00:03<00:15,  3.82it/s]\u001b[A\n"," 20% 15/75 [00:03<00:15,  3.80it/s]\u001b[A\n"," 21% 16/75 [00:03<00:15,  3.81it/s]\u001b[A\n"," 23% 17/75 [00:04<00:15,  3.81it/s]\u001b[A\n"," 24% 18/75 [00:04<00:14,  3.82it/s]\u001b[A\n"," 25% 19/75 [00:04<00:14,  3.81it/s]\u001b[A\n"," 27% 20/75 [00:04<00:14,  3.82it/s]\u001b[A\n"," 28% 21/75 [00:05<00:14,  3.84it/s]\u001b[A\n"," 29% 22/75 [00:05<00:13,  3.85it/s]\u001b[A\n"," 31% 23/75 [00:05<00:13,  3.83it/s]\u001b[A\n"," 32% 24/75 [00:06<00:13,  3.81it/s]\u001b[A\n"," 33% 25/75 [00:06<00:13,  3.82it/s]\u001b[A\n"," 35% 26/75 [00:06<00:12,  3.81it/s]\u001b[A\n"," 36% 27/75 [00:06<00:12,  3.82it/s]\u001b[A\n"," 37% 28/75 [00:07<00:12,  3.81it/s]\u001b[A\n"," 39% 29/75 [00:07<00:12,  3.80it/s]\u001b[A\n"," 40% 30/75 [00:07<00:11,  3.78it/s]\u001b[A\n"," 41% 31/75 [00:07<00:11,  3.77it/s]\u001b[A\n"," 43% 32/75 [00:08<00:11,  3.76it/s]\u001b[A\n"," 44% 33/75 [00:08<00:11,  3.77it/s]\u001b[A\n"," 45% 34/75 [00:08<00:10,  3.78it/s]\u001b[A\n"," 47% 35/75 [00:08<00:10,  3.77it/s]\u001b[A\n"," 48% 36/75 [00:09<00:10,  3.77it/s]\u001b[A\n"," 49% 37/75 [00:09<00:10,  3.78it/s]\u001b[A\n"," 51% 38/75 [00:09<00:09,  3.76it/s]\u001b[A\n"," 52% 39/75 [00:10<00:09,  3.75it/s]\u001b[A\n"," 53% 40/75 [00:10<00:09,  3.75it/s]\u001b[A\n"," 55% 41/75 [00:10<00:09,  3.71it/s]\u001b[A\n"," 56% 42/75 [00:10<00:08,  3.74it/s]\u001b[A\n"," 57% 43/75 [00:11<00:08,  3.72it/s]\u001b[A\n"," 59% 44/75 [00:11<00:08,  3.72it/s]\u001b[A\n"," 60% 45/75 [00:11<00:07,  3.76it/s]\u001b[A\n"," 61% 46/75 [00:11<00:07,  3.77it/s]\u001b[A\n"," 63% 47/75 [00:12<00:07,  3.81it/s]\u001b[A\n"," 64% 48/75 [00:12<00:07,  3.80it/s]\u001b[A\n"," 65% 49/75 [00:12<00:06,  3.80it/s]\u001b[A\n"," 67% 50/75 [00:12<00:06,  3.82it/s]\u001b[A\n"," 68% 51/75 [00:13<00:06,  3.82it/s]\u001b[A\n"," 69% 52/75 [00:13<00:06,  3.79it/s]\u001b[A\n"," 71% 53/75 [00:13<00:05,  3.80it/s]\u001b[A\n"," 72% 54/75 [00:13<00:05,  3.79it/s]\u001b[A\n"," 73% 55/75 [00:14<00:05,  3.82it/s]\u001b[A\n"," 75% 56/75 [00:14<00:04,  3.81it/s]\u001b[A\n"," 76% 57/75 [00:14<00:04,  3.79it/s]\u001b[A\n"," 77% 58/75 [00:15<00:04,  3.79it/s]\u001b[A\n"," 79% 59/75 [00:15<00:04,  3.81it/s]\u001b[A\n"," 80% 60/75 [00:15<00:03,  3.82it/s]\u001b[A\n"," 81% 61/75 [00:15<00:03,  3.80it/s]\u001b[A\n"," 83% 62/75 [00:16<00:03,  3.80it/s]\u001b[A\n"," 84% 63/75 [00:16<00:03,  3.79it/s]\u001b[A\n"," 85% 64/75 [00:16<00:02,  3.79it/s]\u001b[A\n"," 87% 65/75 [00:16<00:02,  3.80it/s]\u001b[A\n"," 88% 66/75 [00:17<00:02,  3.80it/s]\u001b[A\n"," 89% 67/75 [00:17<00:02,  3.80it/s]\u001b[A\n"," 91% 68/75 [00:17<00:01,  3.81it/s]\u001b[A\n"," 92% 69/75 [00:17<00:01,  3.80it/s]\u001b[A\n"," 93% 70/75 [00:18<00:01,  3.80it/s]\u001b[A\n"," 95% 71/75 [00:18<00:01,  3.81it/s]\u001b[A\n"," 96% 72/75 [00:18<00:00,  3.80it/s]\u001b[A\n"," 97% 73/75 [00:18<00:00,  3.79it/s]\u001b[A\n"," 99% 74/75 [00:19<00:00,  3.80it/s]\u001b[A\n","                                      \n","\u001b[A{'eval_loss': 0.2290947437286377, 'eval_precision': 0.5133902003570721, 'eval_recall': 0.5783240223463687, 'eval_f1': 0.5439260193358555, 'eval_accuracy': 0.9248108157991879, 'eval_runtime': 20.9335, 'eval_samples_per_second': 14.331, 'eval_steps_per_second': 3.583, 'epoch': 1.0}\n"," 50% 528/1056 [07:18<05:33,  1.59it/s]\n","100% 75/75 [00:20<00:00,  3.80it/s]\u001b[A\n","{'loss': 0.2324, 'learning_rate': 1.0606060606060608e-06, 'epoch': 1.89}\n","100% 1056/1056 [14:19<00:00,  1.59it/s]\n","  0% 0/75 [00:00<?, ?it/s]\u001b[A\n","  3% 2/75 [00:00<00:09,  7.80it/s]\u001b[A\n","  4% 3/75 [00:00<00:13,  5.29it/s]\u001b[A\n","  5% 4/75 [00:00<00:15,  4.58it/s]\u001b[A\n","  7% 5/75 [00:01<00:16,  4.32it/s]\u001b[A\n","  8% 6/75 [00:01<00:16,  4.17it/s]\u001b[A\n","  9% 7/75 [00:01<00:16,  4.04it/s]\u001b[A\n"," 11% 8/75 [00:01<00:17,  3.93it/s]\u001b[A\n"," 12% 9/75 [00:02<00:16,  3.90it/s]\u001b[A\n"," 13% 10/75 [00:02<00:16,  3.88it/s]\u001b[A\n"," 15% 11/75 [00:02<00:16,  3.86it/s]\u001b[A\n"," 16% 12/75 [00:02<00:16,  3.83it/s]\u001b[A\n"," 17% 13/75 [00:03<00:16,  3.82it/s]\u001b[A\n"," 19% 14/75 [00:03<00:15,  3.83it/s]\u001b[A\n"," 20% 15/75 [00:03<00:15,  3.81it/s]\u001b[A\n"," 21% 16/75 [00:03<00:15,  3.80it/s]\u001b[A\n"," 23% 17/75 [00:04<00:15,  3.80it/s]\u001b[A\n"," 24% 18/75 [00:04<00:14,  3.82it/s]\u001b[A\n"," 25% 19/75 [00:04<00:14,  3.81it/s]\u001b[A\n"," 27% 20/75 [00:04<00:14,  3.83it/s]\u001b[A\n"," 28% 21/75 [00:05<00:14,  3.84it/s]\u001b[A\n"," 29% 22/75 [00:05<00:13,  3.83it/s]\u001b[A\n"," 31% 23/75 [00:05<00:13,  3.82it/s]\u001b[A\n"," 32% 24/75 [00:06<00:13,  3.81it/s]\u001b[A\n"," 33% 25/75 [00:06<00:13,  3.81it/s]\u001b[A\n"," 35% 26/75 [00:06<00:12,  3.81it/s]\u001b[A\n"," 36% 27/75 [00:06<00:12,  3.80it/s]\u001b[A\n"," 37% 28/75 [00:07<00:12,  3.83it/s]\u001b[A\n"," 39% 29/75 [00:07<00:12,  3.80it/s]\u001b[A\n"," 40% 30/75 [00:07<00:11,  3.79it/s]\u001b[A\n"," 41% 31/75 [00:07<00:11,  3.77it/s]\u001b[A\n"," 43% 32/75 [00:08<00:11,  3.75it/s]\u001b[A\n"," 44% 33/75 [00:08<00:11,  3.73it/s]\u001b[A\n"," 45% 34/75 [00:08<00:10,  3.74it/s]\u001b[A\n"," 47% 35/75 [00:08<00:10,  3.78it/s]\u001b[A\n"," 48% 36/75 [00:09<00:10,  3.78it/s]\u001b[A\n"," 49% 37/75 [00:09<00:10,  3.78it/s]\u001b[A\n"," 51% 38/75 [00:09<00:09,  3.75it/s]\u001b[A\n"," 52% 39/75 [00:10<00:09,  3.74it/s]\u001b[A\n"," 53% 40/75 [00:10<00:09,  3.75it/s]\u001b[A\n"," 55% 41/75 [00:10<00:09,  3.72it/s]\u001b[A\n"," 56% 42/75 [00:10<00:08,  3.71it/s]\u001b[A\n"," 57% 43/75 [00:11<00:08,  3.72it/s]\u001b[A\n"," 59% 44/75 [00:11<00:08,  3.71it/s]\u001b[A\n"," 60% 45/75 [00:11<00:08,  3.73it/s]\u001b[A\n"," 61% 46/75 [00:11<00:07,  3.76it/s]\u001b[A\n"," 63% 47/75 [00:12<00:07,  3.76it/s]\u001b[A\n"," 64% 48/75 [00:12<00:07,  3.78it/s]\u001b[A\n"," 65% 49/75 [00:12<00:06,  3.79it/s]\u001b[A\n"," 67% 50/75 [00:12<00:06,  3.78it/s]\u001b[A\n"," 68% 51/75 [00:13<00:06,  3.78it/s]\u001b[A\n"," 69% 52/75 [00:13<00:06,  3.79it/s]\u001b[A\n"," 71% 53/75 [00:13<00:05,  3.81it/s]\u001b[A\n"," 72% 54/75 [00:13<00:05,  3.82it/s]\u001b[A\n"," 73% 55/75 [00:14<00:05,  3.80it/s]\u001b[A\n"," 75% 56/75 [00:14<00:04,  3.80it/s]\u001b[A\n"," 76% 57/75 [00:14<00:04,  3.81it/s]\u001b[A\n"," 77% 58/75 [00:15<00:04,  3.82it/s]\u001b[A\n"," 79% 59/75 [00:15<00:04,  3.81it/s]\u001b[A\n"," 80% 60/75 [00:15<00:03,  3.81it/s]\u001b[A\n"," 81% 61/75 [00:15<00:03,  3.82it/s]\u001b[A\n"," 83% 62/75 [00:16<00:03,  3.82it/s]\u001b[A\n"," 84% 63/75 [00:16<00:03,  3.80it/s]\u001b[A\n"," 85% 64/75 [00:16<00:02,  3.79it/s]\u001b[A\n"," 87% 65/75 [00:16<00:02,  3.77it/s]\u001b[A\n"," 88% 66/75 [00:17<00:02,  3.81it/s]\u001b[A\n"," 89% 67/75 [00:17<00:02,  3.78it/s]\u001b[A\n"," 91% 68/75 [00:17<00:01,  3.80it/s]\u001b[A\n"," 92% 69/75 [00:17<00:01,  3.80it/s]\u001b[A\n"," 93% 70/75 [00:18<00:01,  3.81it/s]\u001b[A\n"," 95% 71/75 [00:18<00:01,  3.80it/s]\u001b[A\n"," 96% 72/75 [00:18<00:00,  3.76it/s]\u001b[A\n"," 97% 73/75 [00:19<00:00,  3.76it/s]\u001b[A\n"," 99% 74/75 [00:19<00:00,  3.77it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.15681307017803192, 'eval_precision': 0.7698726394378568, 'eval_recall': 0.783463687150838, 'eval_f1': 0.7766087052829771, 'eval_accuracy': 0.955657069029162, 'eval_runtime': 20.93, 'eval_samples_per_second': 14.333, 'eval_steps_per_second': 3.583, 'epoch': 2.0}\n","100% 1056/1056 [14:40<00:00,  1.59it/s]\n","100% 75/75 [00:20<00:00,  3.78it/s]\u001b[A\n","{'train_runtime': 886.8577, 'train_samples_per_second': 4.756, 'train_steps_per_second': 1.191, 'train_loss': 0.30285329439423303, 'epoch': 2.0}\n","100% 1056/1056 [14:46<00:00,  1.19it/s]\n","100% 151/151 [00:43<00:00,  3.48it/s]\n","\n","\n"," ---RESULTS FOR THE TEST DATASET:--- \n","\n","DATE : \n"," {'precision': 0.7970204841713222, 'recall': 0.8113744075829384, 'f1': 0.8041333959605449, 'number': 1055} \n","\n","LOC : \n"," {'precision': 0.6948971413304835, 'recall': 0.8317876559002239, 'f1': 0.7572052401746725, 'number': 3127} \n","\n","ORG : \n"," {'precision': 0.5545977011494253, 'recall': 0.15845648604269294, 'f1': 0.24648786717752239, 'number': 1218} \n","\n","PER : \n"," {'precision': 0.6395961369622476, 'recall': 0.6862929816297691, 'f1': 0.6621222449443308, 'number': 2123} \n","\n","overall_precision : \n"," 0.6861480585785301 \n","\n","overall_recall : \n"," 0.6788515219992024 \n","\n","overall_f1 : \n"," 0.6824802886542829 \n","\n","overall_accuracy : \n"," 0.9490177633881695 \n","\n"]}]}]}