{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOvOrxhgWZnEYOUdb1VyM7H"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"I9jCjsexmnU2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683578582256,"user_tz":-120,"elapsed":230,"user":{"displayName":"Javier Ramos GutiÃ©rrez","userId":"09808428157736141755"}},"outputId":"54cbe5c3-dfbf-429f-a995-3374e33c0485"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/NLP\n"]}],"source":["#Upload the folder manually\n","%cd /content/NLP"]},{"cell_type":"code","source":["%pip install transformers datasets seqeval"],"metadata":{"id":"4ZKQoB8XnklA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683578607369,"user_tz":-120,"elapsed":23119,"user":{"displayName":"Javier Ramos GutiÃ©rrez","userId":"09808428157736141755"}},"outputId":"f8360488-1906-4724-e62a-326bc9c75c2f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting xxhash\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=c1ba0e458c07259392e5d5f92498647a9764d2b0c12c28343783d3947a9adf9b\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: tokenizers, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, seqeval, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 seqeval-1.2.2 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n"]}]},{"cell_type":"code","source":["!python3 mbert.py --lang=ibo"],"metadata":{"id":"LOAXh9OInn20","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683578745685,"user_tz":-120,"elapsed":138324,"user":{"displayName":"Javier Ramos GutiÃ©rrez","userId":"09808428157736141755"}},"outputId":"1a8bbb18-e2fc-4252-c7ba-9256a49705a0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-08 20:43:35.856472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Downloading builder script: 100% 7.60k/7.60k [00:00<00:00, 5.58MB/s]\n","Downloading metadata: 100% 39.4k/39.4k [00:00<00:00, 15.1MB/s]\n","Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 9.79MB/s]\n","Downloading and preparing dataset masakhaner/ibo to /root/.cache/huggingface/datasets/masakhaner/ibo/1.0.0/e61b24903076a3af7682855beebb820ec64edad0d6787b148c473694592d10b3...\n","Downloading data files:   0% 0/3 [00:00<?, ?it/s]\n","Downloading data: 355kB [00:00, 21.4MB/s]        \n","Downloading data files:  33% 1/3 [00:00<00:01,  1.39it/s]\n","Downloading data: 53.7kB [00:00, 18.2MB/s]       \n","Downloading data files:  67% 2/3 [00:01<00:00,  1.63it/s]\n","Downloading data: 107kB [00:00, 27.1MB/s]        \n","Downloading data files: 100% 3/3 [00:01<00:00,  1.66it/s]\n","Extracting data files: 100% 3/3 [00:00<00:00, 1272.03it/s]\n","Dataset masakhaner downloaded and prepared to /root/.cache/huggingface/datasets/masakhaner/ibo/1.0.0/e61b24903076a3af7682855beebb820ec64edad0d6787b148c473694592d10b3. Subsequent calls will reuse this data.\n","100% 3/3 [00:00<00:00, 204.54it/s]\n","['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE']\n","Downloading (â€¦)okenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 107kB/s]\n","Downloading (â€¦)lve/main/config.json: 100% 625/625 [00:00<00:00, 2.44MB/s]\n","Downloading (â€¦)solve/main/vocab.txt: 100% 996k/996k [00:00<00:00, 16.2MB/s]\n","Downloading (â€¦)/main/tokenizer.json: 100% 1.96M/1.96M [00:00<00:00, 26.2MB/s]\n","Downloading pytorch_model.bin: 100% 714M/714M [00:03<00:00, 202MB/s]\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/content/NLP/mbert.py:76: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"seqeval\")\n","Downloading builder script: 6.33kB [00:00, 3.39MB/s]       \n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","  0% 0/280 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"," 50% 140/280 [00:41<00:40,  3.43it/s]\n","  0% 0/20 [00:00<?, ?it/s]\u001b[A\n"," 15% 3/20 [00:00<00:00, 20.99it/s]\u001b[A\n"," 30% 6/20 [00:00<00:00, 16.60it/s]\u001b[A\n"," 40% 8/20 [00:00<00:00, 15.07it/s]\u001b[A\n"," 50% 10/20 [00:00<00:00, 14.51it/s]\u001b[A\n"," 60% 12/20 [00:00<00:00, 14.79it/s]\u001b[A\n"," 70% 14/20 [00:00<00:00, 13.44it/s]\u001b[A\n"," 80% 16/20 [00:01<00:00, 12.08it/s]\u001b[A\n"," 90% 18/20 [00:01<00:00, 12.83it/s]\u001b[A\n","100% 20/20 [00:01<00:00, 12.53it/s]\u001b[A\n","{'eval_loss': 0.15788264572620392, 'eval_precision': 0.7759472817133443, 'eval_recall': 0.785, 'eval_f1': 0.7804473902236951, 'eval_accuracy': 0.9546319796954315, 'eval_runtime': 1.8009, 'eval_samples_per_second': 177.694, 'eval_steps_per_second': 11.106, 'epoch': 1.0}\n","\n"," 50% 140/280 [00:43<00:40,  3.43it/s]\n","100% 280/280 [01:31<00:00,  3.73it/s]\n","  0% 0/20 [00:00<?, ?it/s]\u001b[A\n"," 15% 3/20 [00:00<00:00, 20.30it/s]\u001b[A\n"," 30% 6/20 [00:00<00:00, 16.39it/s]\u001b[A\n"," 40% 8/20 [00:00<00:00, 14.96it/s]\u001b[A\n"," 50% 10/20 [00:00<00:00, 14.51it/s]\u001b[A\n"," 60% 12/20 [00:00<00:00, 15.00it/s]\u001b[A\n"," 70% 14/20 [00:00<00:00, 13.63it/s]\u001b[A\n"," 80% 16/20 [00:01<00:00, 12.24it/s]\u001b[A\n"," 90% 18/20 [00:01<00:00, 13.03it/s]\u001b[A\n","100% 20/20 [00:01<00:00, 12.66it/s]\u001b[A\n","{'eval_loss': 0.12952116131782532, 'eval_precision': 0.8045234248788369, 'eval_recall': 0.83, 'eval_f1': 0.8170631665299427, 'eval_accuracy': 0.963991116751269, 'eval_runtime': 1.6555, 'eval_samples_per_second': 193.298, 'eval_steps_per_second': 12.081, 'epoch': 2.0}\n","\n","100% 280/280 [01:33<00:00,  3.73it/s]\n","{'train_runtime': 102.6978, 'train_samples_per_second': 43.526, 'train_steps_per_second': 2.726, 'train_loss': 0.20176749910627093, 'epoch': 2.0}\n","100% 280/280 [01:42<00:00,  2.73it/s]\n","100% 40/40 [00:03<00:00, 11.71it/s]\n","\n","\n"," ---RESULTS FOR THE TEST DATASET:--- \n","\n","DATE : \n"," {'precision': 0.5789473684210527, 'recall': 0.6068965517241379, 'f1': 0.5925925925925927, 'number': 145} \n","\n","LOC : \n"," {'precision': 0.71875, 'recall': 0.8165680473372781, 'f1': 0.7645429362880886, 'number': 338} \n","\n","ORG : \n"," {'precision': 0.8375796178343949, 'recall': 0.7492877492877493, 'f1': 0.7909774436090227, 'number': 351} \n","\n","PER : \n"," {'precision': 0.8011204481792717, 'recall': 0.8289855072463768, 'f1': 0.8148148148148148, 'number': 345} \n","\n","overall_precision : \n"," 0.7564208782104391 \n","\n","overall_recall : \n"," 0.7743850720949957 \n","\n","overall_f1 : \n"," 0.7652975691533949 \n","\n","overall_accuracy : \n"," 0.9606959272439699 \n","\n"]}]},{"cell_type":"code","source":["!python3 canine.py --lang=ibo"],"metadata":{"id":"HpxYTarlnqMD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683579748369,"user_tz":-120,"elapsed":1002713,"user":{"displayName":"Javier Ramos GutiÃ©rrez","userId":"09808428157736141755"}},"outputId":"1662df52-2656-421f-f6fc-ecc10a40d2bf"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-05-08 20:45:53.759230: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Found cached dataset masakhaner (/root/.cache/huggingface/datasets/masakhaner/ibo/1.0.0/e61b24903076a3af7682855beebb820ec64edad0d6787b148c473694592d10b3)\n","100% 3/3 [00:00<00:00, 653.25it/s]\n","['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE']\n","Downloading (â€¦)cial_tokens_map.json: 100% 657/657 [00:00<00:00, 2.76MB/s]\n","Downloading (â€¦)okenizer_config.json: 100% 892/892 [00:00<00:00, 5.06MB/s]\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Using unk_token, but it is not set yet.\n","Downloading (â€¦)lve/main/config.json: 100% 698/698 [00:00<00:00, 2.80MB/s]\n","Downloading pytorch_model.bin: 100% 529M/529M [00:02<00:00, 196MB/s]\n","Some weights of CanineForTokenClassification were not initialized from the model checkpoint at google/canine-c and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/content/NLP/canine.py:93: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"seqeval\")\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","{'loss': 0.4383, 'learning_rate': 1.1055456171735241e-05, 'epoch': 0.89}\n"," 50% 559/1118 [07:18<06:46,  1.38it/s]\n","  0% 0/80 [00:00<?, ?it/s]\u001b[A\n","  2% 2/80 [00:00<00:10,  7.62it/s]\u001b[A\n","  4% 3/80 [00:00<00:14,  5.39it/s]\u001b[A\n","  5% 4/80 [00:00<00:16,  4.59it/s]\u001b[A\n","  6% 5/80 [00:01<00:17,  4.32it/s]\u001b[A\n","  8% 6/80 [00:01<00:17,  4.15it/s]\u001b[A\n","  9% 7/80 [00:01<00:18,  4.04it/s]\u001b[A\n"," 10% 8/80 [00:01<00:18,  3.92it/s]\u001b[A\n"," 11% 9/80 [00:02<00:18,  3.89it/s]\u001b[A\n"," 12% 10/80 [00:02<00:18,  3.87it/s]\u001b[A\n"," 14% 11/80 [00:02<00:18,  3.81it/s]\u001b[A\n"," 15% 12/80 [00:02<00:17,  3.83it/s]\u001b[A\n"," 16% 13/80 [00:03<00:17,  3.81it/s]\u001b[A\n"," 18% 14/80 [00:03<00:17,  3.80it/s]\u001b[A\n"," 19% 15/80 [00:03<00:17,  3.79it/s]\u001b[A\n"," 20% 16/80 [00:03<00:16,  3.83it/s]\u001b[A\n"," 21% 17/80 [00:04<00:16,  3.83it/s]\u001b[A\n"," 22% 18/80 [00:04<00:16,  3.84it/s]\u001b[A\n"," 24% 19/80 [00:04<00:15,  3.86it/s]\u001b[A\n"," 25% 20/80 [00:04<00:15,  3.85it/s]\u001b[A\n"," 26% 21/80 [00:05<00:15,  3.84it/s]\u001b[A\n"," 28% 22/80 [00:05<00:15,  3.84it/s]\u001b[A\n"," 29% 23/80 [00:05<00:14,  3.84it/s]\u001b[A\n"," 30% 24/80 [00:06<00:14,  3.85it/s]\u001b[A\n"," 31% 25/80 [00:06<00:14,  3.84it/s]\u001b[A\n"," 32% 26/80 [00:06<00:13,  3.86it/s]\u001b[A\n"," 34% 27/80 [00:06<00:13,  3.86it/s]\u001b[A\n"," 35% 28/80 [00:07<00:13,  3.85it/s]\u001b[A\n"," 36% 29/80 [00:07<00:13,  3.85it/s]\u001b[A\n"," 38% 30/80 [00:07<00:12,  3.86it/s]\u001b[A\n"," 39% 31/80 [00:07<00:12,  3.85it/s]\u001b[A\n"," 40% 32/80 [00:08<00:12,  3.83it/s]\u001b[A\n"," 41% 33/80 [00:08<00:12,  3.84it/s]\u001b[A\n"," 42% 34/80 [00:08<00:11,  3.84it/s]\u001b[A\n"," 44% 35/80 [00:08<00:11,  3.84it/s]\u001b[A\n"," 45% 36/80 [00:09<00:11,  3.85it/s]\u001b[A\n"," 46% 37/80 [00:09<00:11,  3.85it/s]\u001b[A\n"," 48% 38/80 [00:09<00:10,  3.87it/s]\u001b[A\n"," 49% 39/80 [00:09<00:10,  3.86it/s]\u001b[A\n"," 50% 40/80 [00:10<00:10,  3.85it/s]\u001b[A\n"," 51% 41/80 [00:10<00:10,  3.84it/s]\u001b[A\n"," 52% 42/80 [00:10<00:09,  3.84it/s]\u001b[A\n"," 54% 43/80 [00:10<00:09,  3.84it/s]\u001b[A\n"," 55% 44/80 [00:11<00:09,  3.85it/s]\u001b[A\n"," 56% 45/80 [00:11<00:09,  3.87it/s]\u001b[A\n"," 57% 46/80 [00:11<00:08,  3.86it/s]\u001b[A\n"," 59% 47/80 [00:12<00:08,  3.86it/s]\u001b[A\n"," 60% 48/80 [00:12<00:08,  3.87it/s]\u001b[A\n"," 61% 49/80 [00:12<00:08,  3.86it/s]\u001b[A\n"," 62% 50/80 [00:12<00:07,  3.86it/s]\u001b[A\n"," 64% 51/80 [00:13<00:07,  3.85it/s]\u001b[A\n"," 65% 52/80 [00:13<00:07,  3.83it/s]\u001b[A\n"," 66% 53/80 [00:13<00:07,  3.82it/s]\u001b[A\n"," 68% 54/80 [00:13<00:06,  3.81it/s]\u001b[A\n"," 69% 55/80 [00:14<00:06,  3.80it/s]\u001b[A\n"," 70% 56/80 [00:14<00:06,  3.80it/s]\u001b[A\n"," 71% 57/80 [00:14<00:06,  3.78it/s]\u001b[A\n"," 72% 58/80 [00:14<00:05,  3.80it/s]\u001b[A\n"," 74% 59/80 [00:15<00:05,  3.81it/s]\u001b[A\n"," 75% 60/80 [00:15<00:05,  3.80it/s]\u001b[A\n"," 76% 61/80 [00:15<00:05,  3.78it/s]\u001b[A\n"," 78% 62/80 [00:15<00:04,  3.79it/s]\u001b[A\n"," 79% 63/80 [00:16<00:04,  3.82it/s]\u001b[A\n"," 80% 64/80 [00:16<00:04,  3.83it/s]\u001b[A\n"," 81% 65/80 [00:16<00:03,  3.79it/s]\u001b[A\n"," 82% 66/80 [00:16<00:03,  3.81it/s]\u001b[A\n"," 84% 67/80 [00:17<00:03,  3.80it/s]\u001b[A\n"," 85% 68/80 [00:17<00:03,  3.80it/s]\u001b[A\n"," 86% 69/80 [00:17<00:02,  3.80it/s]\u001b[A\n"," 88% 70/80 [00:18<00:02,  3.79it/s]\u001b[A\n"," 89% 71/80 [00:18<00:02,  3.80it/s]\u001b[A\n"," 90% 72/80 [00:18<00:02,  3.80it/s]\u001b[A\n"," 91% 73/80 [00:18<00:01,  3.80it/s]\u001b[A\n"," 92% 74/80 [00:19<00:01,  3.80it/s]\u001b[A\n"," 94% 75/80 [00:19<00:01,  3.81it/s]\u001b[A\n"," 95% 76/80 [00:19<00:01,  3.83it/s]\u001b[A\n"," 96% 77/80 [00:19<00:00,  3.83it/s]\u001b[A\n"," 98% 78/80 [00:20<00:00,  3.85it/s]\u001b[A\n"," 99% 79/80 [00:20<00:00,  3.85it/s]\u001b[A\n","                                      \n","\u001b[A{'eval_loss': 0.3135777711868286, 'eval_precision': 0.6853644646924829, 'eval_recall': 0.6394792773645058, 'eval_f1': 0.6616272677295217, 'eval_accuracy': 0.9190407263635766, 'eval_runtime': 21.8448, 'eval_samples_per_second': 14.649, 'eval_steps_per_second': 3.662, 'epoch': 1.0}\n"," 50% 559/1118 [07:40<06:46,  1.38it/s]\n","100% 80/80 [00:21<00:00,  3.85it/s]\u001b[A\n","{'loss': 0.2324, 'learning_rate': 2.110912343470483e-06, 'epoch': 1.79}\n","100% 1118/1118 [15:03<00:00,  1.37it/s]\n","  0% 0/80 [00:00<?, ?it/s]\u001b[A\n","  2% 2/80 [00:00<00:10,  7.69it/s]\u001b[A\n","  4% 3/80 [00:00<00:14,  5.43it/s]\u001b[A\n","  5% 4/80 [00:00<00:16,  4.66it/s]\u001b[A\n","  6% 5/80 [00:01<00:17,  4.37it/s]\u001b[A\n","  8% 6/80 [00:01<00:17,  4.18it/s]\u001b[A\n","  9% 7/80 [00:01<00:17,  4.07it/s]\u001b[A\n"," 10% 8/80 [00:01<00:18,  3.98it/s]\u001b[A\n"," 11% 9/80 [00:02<00:18,  3.94it/s]\u001b[A\n"," 12% 10/80 [00:02<00:17,  3.92it/s]\u001b[A\n"," 14% 11/80 [00:02<00:17,  3.90it/s]\u001b[A\n"," 15% 12/80 [00:02<00:17,  3.86it/s]\u001b[A\n"," 16% 13/80 [00:03<00:17,  3.85it/s]\u001b[A\n"," 18% 14/80 [00:03<00:17,  3.86it/s]\u001b[A\n"," 19% 15/80 [00:03<00:16,  3.86it/s]\u001b[A\n"," 20% 16/80 [00:03<00:16,  3.85it/s]\u001b[A\n"," 21% 17/80 [00:04<00:16,  3.85it/s]\u001b[A\n"," 22% 18/80 [00:04<00:16,  3.84it/s]\u001b[A\n"," 24% 19/80 [00:04<00:15,  3.84it/s]\u001b[A\n"," 25% 20/80 [00:04<00:15,  3.84it/s]\u001b[A\n"," 26% 21/80 [00:05<00:15,  3.82it/s]\u001b[A\n"," 28% 22/80 [00:05<00:15,  3.82it/s]\u001b[A\n"," 29% 23/80 [00:05<00:14,  3.82it/s]\u001b[A\n"," 30% 24/80 [00:05<00:14,  3.82it/s]\u001b[A\n"," 31% 25/80 [00:06<00:14,  3.80it/s]\u001b[A\n"," 32% 26/80 [00:06<00:14,  3.80it/s]\u001b[A\n"," 34% 27/80 [00:06<00:13,  3.81it/s]\u001b[A\n"," 35% 28/80 [00:07<00:13,  3.81it/s]\u001b[A\n"," 36% 29/80 [00:07<00:13,  3.80it/s]\u001b[A\n"," 38% 30/80 [00:07<00:13,  3.82it/s]\u001b[A\n"," 39% 31/80 [00:07<00:12,  3.81it/s]\u001b[A\n"," 40% 32/80 [00:08<00:12,  3.76it/s]\u001b[A\n"," 41% 33/80 [00:08<00:12,  3.74it/s]\u001b[A\n"," 42% 34/80 [00:08<00:12,  3.75it/s]\u001b[A\n"," 44% 35/80 [00:08<00:12,  3.75it/s]\u001b[A\n"," 45% 36/80 [00:09<00:11,  3.76it/s]\u001b[A\n"," 46% 37/80 [00:09<00:11,  3.78it/s]\u001b[A\n"," 48% 38/80 [00:09<00:11,  3.81it/s]\u001b[A\n"," 49% 39/80 [00:09<00:10,  3.81it/s]\u001b[A\n"," 50% 40/80 [00:10<00:10,  3.80it/s]\u001b[A\n"," 51% 41/80 [00:10<00:10,  3.81it/s]\u001b[A\n"," 52% 42/80 [00:10<00:09,  3.82it/s]\u001b[A\n"," 54% 43/80 [00:11<00:09,  3.82it/s]\u001b[A\n"," 55% 44/80 [00:11<00:09,  3.82it/s]\u001b[A\n"," 56% 45/80 [00:11<00:09,  3.83it/s]\u001b[A\n"," 57% 46/80 [00:11<00:08,  3.85it/s]\u001b[A\n"," 59% 47/80 [00:12<00:08,  3.83it/s]\u001b[A\n"," 60% 48/80 [00:12<00:08,  3.82it/s]\u001b[A\n"," 61% 49/80 [00:12<00:08,  3.83it/s]\u001b[A\n"," 62% 50/80 [00:12<00:07,  3.82it/s]\u001b[A\n"," 64% 51/80 [00:13<00:07,  3.83it/s]\u001b[A\n"," 65% 52/80 [00:13<00:07,  3.82it/s]\u001b[A\n"," 66% 53/80 [00:13<00:07,  3.84it/s]\u001b[A\n"," 68% 54/80 [00:13<00:06,  3.86it/s]\u001b[A\n"," 69% 55/80 [00:14<00:06,  3.85it/s]\u001b[A\n"," 70% 56/80 [00:14<00:06,  3.85it/s]\u001b[A\n"," 71% 57/80 [00:14<00:05,  3.84it/s]\u001b[A\n"," 72% 58/80 [00:14<00:05,  3.85it/s]\u001b[A\n"," 74% 59/80 [00:15<00:05,  3.83it/s]\u001b[A\n"," 75% 60/80 [00:15<00:05,  3.84it/s]\u001b[A\n"," 76% 61/80 [00:15<00:04,  3.85it/s]\u001b[A\n"," 78% 62/80 [00:15<00:04,  3.86it/s]\u001b[A\n"," 79% 63/80 [00:16<00:04,  3.86it/s]\u001b[A\n"," 80% 64/80 [00:16<00:04,  3.85it/s]\u001b[A\n"," 81% 65/80 [00:16<00:03,  3.85it/s]\u001b[A\n"," 82% 66/80 [00:16<00:03,  3.87it/s]\u001b[A\n"," 84% 67/80 [00:17<00:03,  3.86it/s]\u001b[A\n"," 85% 68/80 [00:17<00:03,  3.86it/s]\u001b[A\n"," 86% 69/80 [00:17<00:02,  3.85it/s]\u001b[A\n"," 88% 70/80 [00:18<00:02,  3.85it/s]\u001b[A\n"," 89% 71/80 [00:18<00:02,  3.85it/s]\u001b[A\n"," 90% 72/80 [00:18<00:02,  3.84it/s]\u001b[A\n"," 91% 73/80 [00:18<00:01,  3.86it/s]\u001b[A\n"," 92% 74/80 [00:19<00:01,  3.85it/s]\u001b[A\n"," 94% 75/80 [00:19<00:01,  3.85it/s]\u001b[A\n"," 95% 76/80 [00:19<00:01,  3.84it/s]\u001b[A\n"," 96% 77/80 [00:19<00:00,  3.84it/s]\u001b[A\n"," 98% 78/80 [00:20<00:00,  3.83it/s]\u001b[A\n"," 99% 79/80 [00:20<00:00,  3.83it/s]\u001b[A\n","                                       \n","\u001b[A{'eval_loss': 0.23917117714881897, 'eval_precision': 0.7463845741831816, 'eval_recall': 0.7404357066950054, 'eval_f1': 0.7433982395305415, 'eval_accuracy': 0.9360484242384367, 'eval_runtime': 22.6738, 'eval_samples_per_second': 14.113, 'eval_steps_per_second': 3.528, 'epoch': 2.0}\n","100% 1118/1118 [15:25<00:00,  1.37it/s]\n","100% 80/80 [00:22<00:00,  3.82it/s]\u001b[A\n","{'train_runtime': 932.4365, 'train_samples_per_second': 4.794, 'train_steps_per_second': 1.199, 'train_loss': 0.32142751929158603, 'epoch': 2.0}\n","100% 1118/1118 [15:32<00:00,  1.20it/s]\n","100% 160/160 [00:43<00:00,  3.67it/s]\n","\n","\n"," ---RESULTS FOR THE TEST DATASET:--- \n","\n","DATE : \n"," {'precision': 0.40357598978288634, 'recall': 0.3319327731092437, 'f1': 0.3642651296829971, 'number': 952} \n","\n","LOC : \n"," {'precision': 0.6934984520123839, 'recall': 0.8331008833100884, 'f1': 0.7569165786694826, 'number': 2151} \n","\n","ORG : \n"," {'precision': 0.7019758507135017, 'recall': 0.7189432265317595, 'f1': 0.7103582338239378, 'number': 1779} \n","\n","PER : \n"," {'precision': 0.7165096272019664, 'recall': 0.776988005330964, 'f1': 0.7455242966751918, 'number': 2251} \n","\n","overall_precision : \n"," 0.673132372214941 \n","\n","overall_recall : \n"," 0.7200336464320762 \n","\n","overall_f1 : \n"," 0.695793537898801 \n","\n","overall_accuracy : \n"," 0.933241636783416 \n","\n"]}]}]}