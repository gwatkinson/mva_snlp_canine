{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets evaluate sacrebleu","metadata":{"execution":{"iopub.status.busy":"2023-05-08T12:46:03.386851Z","iopub.execute_input":"2023-05-08T12:46:03.387502Z","iopub.status.idle":"2023-05-08T12:46:20.018582Z","shell.execute_reply.started":"2023-05-08T12:46:03.387471Z","shell.execute_reply":"2023-05-08T12:46:20.017347Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sacrebleu\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2023.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.3)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (2.7.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.7/site-packages (from sacrebleu) (4.9.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (22.2.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.8.2)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.4.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.7.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\nInstalling collected packages: sacrebleu, evaluate\nSuccessfully installed evaluate-0.4.0 sacrebleu-2.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-05-08T12:46:20.022201Z","iopub.execute_input":"2023-05-08T12:46:20.022547Z","iopub.status.idle":"2023-05-08T12:46:20.028091Z","shell.execute_reply.started":"2023-05-08T12:46:20.022515Z","shell.execute_reply":"2023-05-08T12:46:20.027105Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Global imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport torch\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-08T12:46:20.029905Z","iopub.execute_input":"2023-05-08T12:46:20.030283Z","iopub.status.idle":"2023-05-08T12:46:24.354579Z","shell.execute_reply.started":"2023-05-08T12:46:20.030246Z","shell.execute_reply":"2023-05-08T12:46:24.353546Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-05-08T12:46:24.359031Z","iopub.execute_input":"2023-05-08T12:46:24.359932Z","iopub.status.idle":"2023-05-08T12:46:24.468177Z","shell.execute_reply.started":"2023-05-08T12:46:24.359880Z","shell.execute_reply":"2023-05-08T12:46:24.466821Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# # https://huggingface.co/datasets/wmt14\n# from datasets import load_dataset\n\n# fren_ds = load_dataset(\"wmt14\", 'fr-en')\nbooks = load_dataset(\"opus_books\", \"en-fr\")\nbooks = books[\"train\"].train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.539886Z","iopub.execute_input":"2023-05-01T15:16:04.542098Z","iopub.status.idle":"2023-05-01T15:16:04.666334Z","shell.execute_reply.started":"2023-05-01T15:16:04.542064Z","shell.execute_reply":"2023-05-01T15:16:04.661330Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2038114043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from datasets import load_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfren_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wmt14\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fr-en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"opus_books\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en-fr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"],"ename":"NameError","evalue":"name 'load_dataset' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# CANINE: Translation Task","metadata":{}},{"cell_type":"markdown","source":"As CANINE isn't made for Seq2Seq tasks, we have to use it another way. We decided to use CANINE as an encoder, to use its features, and stack a Decoder for Translation on it.\n\nIn order to do that, we used the EncoderDecoder of HuggingFace, in order to use another pre-trained decoder.\nThe resulting model being too large, we weren't able to tran it on our computers, on kaggle or on google colab.\n\nThus, we had to freeze the layers of either the encoder (Canine) or the decoder. We choose to freeze Canine for several reasons:\n* The decoder is pre-trained with its tokenizer. Since Canine's objective is to get rid of any tokenizer, we have to fine-tune the decoder to make it forget its embedding.\n* CANINE's papers prones its pre-trained features, then it's coherent to use it without fine-tunning.\n\nNote that we have to adapt the embedding dimension of the decoder since CANINE is vocabulary free.","metadata":{}},{"cell_type":"markdown","source":"## Using BART - For English to French (on Books)","metadata":{}},{"cell_type":"markdown","source":"To familiarize with the architecture, we first try to fine-tune our model on the Books dataset, to translate from English to French.\n\nResults were not pretty good but as good to glimpse a possibility of being good on translation.","metadata":{}},{"cell_type":"markdown","source":"### Model configuration","metadata":{}},{"cell_type":"markdown","source":"Load model","metadata":{}},{"cell_type":"code","source":"from transformers import CanineTokenizer, BartTokenizer, CanineConfig, BartConfig, EncoderDecoderConfig, EncoderDecoderModel\n\nconfig_encoder = CanineConfig().from_pretrained(\"google/canine-c\")\nconfig_decoder = BartConfig().from_pretrained(\"facebook/bart-base\")\nconfig_decoder.vocab_size = config_encoder.eos_token_id+1\nconfig = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n\ntokenizer_encoder = CanineTokenizer.from_pretrained(\"google/canine-c\")\ntokenizer_decoder = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n\nmodel = EncoderDecoderModel(config).to(device)\nmodel.config.decoder_start_token_id = model.decoder.config.decoder_start_token_id\nmodel.config.pad_token_id = model.decoder.config.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2023-05-08T12:46:24.470483Z","iopub.execute_input":"2023-05-08T12:46:24.470925Z","iopub.status.idle":"2023-05-08T12:46:57.763437Z","shell.execute_reply.started":"2023-05-08T12:46:24.470884Z","shell.execute_reply":"2023-05-08T12:46:57.762227Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/698 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"706aa5a16d2f4dee916406af7981f267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3586580b9494a4ab29b829ec446d06c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/657 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3164710e365e4d6bae784f308e96bea8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c42ffa1a31814fecb227fc92ad6e4cac"}},"metadata":{}},{"name":"stderr","text":"Using unk_token, but it is not set yet.\nUsing unk_token, but it is not set yet.\nUsing unk_token, but it is not set yet.\nUsing unk_token, but it is not set yet.\nUsing unk_token, but it is not set yet.\nUsing unk_token, but it is not set yet.\nUsing unk_token, but it is not set yet.\nUsing unk_token, but it is not set yet.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d8fca47c714da2a52a4bb8854fff7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c13fe39eae46ab910fdfbcff022fb9"}},"metadata":{}},{"name":"stderr","text":"Config of the decoder: <class 'transformers.models.bart.modeling_bart.BartForCausalLM'> is overwritten by shared decoder config: BartConfig {\n  \"_name_or_path\": \"bart-base\",\n  \"activation_dropout\": 0.1,\n  \"activation_function\": \"gelu\",\n  \"add_bias_logits\": false,\n  \"add_cross_attention\": true,\n  \"add_final_layer_norm\": false,\n  \"architectures\": [\n    \"BartModel\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"bos_token_id\": 0,\n  \"classif_dropout\": 0.1,\n  \"classifier_dropout\": 0.0,\n  \"d_model\": 768,\n  \"decoder_attention_heads\": 12,\n  \"decoder_ffn_dim\": 3072,\n  \"decoder_layerdrop\": 0.0,\n  \"decoder_layers\": 6,\n  \"decoder_start_token_id\": 2,\n  \"dropout\": 0.1,\n  \"early_stopping\": true,\n  \"encoder_attention_heads\": 12,\n  \"encoder_ffn_dim\": 3072,\n  \"encoder_layerdrop\": 0.0,\n  \"encoder_layers\": 6,\n  \"eos_token_id\": 2,\n  \"forced_bos_token_id\": 0,\n  \"forced_eos_token_id\": 2,\n  \"gradient_checkpointing\": false,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"init_std\": 0.02,\n  \"is_decoder\": true,\n  \"is_encoder_decoder\": true,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"max_position_embeddings\": 1024,\n  \"model_type\": \"bart\",\n  \"no_repeat_ngram_size\": 3,\n  \"normalize_before\": false,\n  \"normalize_embedding\": true,\n  \"num_beams\": 4,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"scale_embedding\": false,\n  \"task_specific_params\": {\n    \"summarization\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 128,\n      \"min_length\": 12,\n      \"num_beams\": 4\n    },\n    \"summarization_cnn\": {\n      \"length_penalty\": 2.0,\n      \"max_length\": 142,\n      \"min_length\": 56,\n      \"num_beams\": 4\n    },\n    \"summarization_xsum\": {\n      \"length_penalty\": 1.0,\n      \"max_length\": 62,\n      \"min_length\": 11,\n      \"num_beams\": 6\n    }\n  },\n  \"torch_dtype\": \"float32\",\n  \"transformers_version\": \"4.26.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 57346\n}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Freeze encoder","metadata":{}},{"cell_type":"code","source":"for param in model.encoder.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-05-08T12:46:57.765582Z","iopub.execute_input":"2023-05-08T12:46:57.766029Z","iopub.status.idle":"2023-05-08T12:46:57.775080Z","shell.execute_reply.started":"2023-05-08T12:46:57.765988Z","shell.execute_reply":"2023-05-08T12:46:57.774130Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Data loading","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [example[source_lang] for example in examples[\"translation\"]]\n    targets = [example[target_lang] for example in examples[\"translation\"]]\n    model_inputs = tokenizer_encoder(inputs, text_target=targets, max_length=128, truncation=True, padding=\"max_length\")\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2023-05-08T12:46:57.776740Z","iopub.execute_input":"2023-05-08T12:46:57.777244Z","iopub.status.idle":"2023-05-08T12:46:57.784551Z","shell.execute_reply.started":"2023-05-08T12:46:57.777200Z","shell.execute_reply":"2023-05-08T12:46:57.783249Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\nsource_lang = \"en\"\ntarget_lang = \"fr\"\n\nbooks = load_dataset(\"opus_books\", \"en-fr\")\nbooks = books[\"train\"].train_test_split(test_size=0.2)\n\ntokenized_books = books.map(preprocess_function, batched=True)\ntokenized_books = tokenized_books.remove_columns([\"translation\", \"id\"])\ntokenized_books.set_format(\"torch\")\n\nsmall_train_dataset = tokenized_books[\"train\"].shuffle(seed=42)#.select(range(1000))\nsmall_eval_dataset = tokenized_books[\"test\"].shuffle(seed=42)#.select(range(1000))\n\ntrain_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\neval_dataloader = DataLoader(small_eval_dataset, batch_size=8)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T12:46:57.786249Z","iopub.execute_input":"2023-05-08T12:46:57.786684Z","iopub.status.idle":"2023-05-08T12:48:08.006600Z","shell.execute_reply.started":"2023-05-08T12:46:57.786642Z","shell.execute_reply":"2023-05-08T12:48:08.005480Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f9d1e8466a943e19d4bbd6ead4381be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7786f246a8214c86a7645f5d5288c541"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset opus_books/en-fr (download: 11.45 MiB, generated: 31.47 MiB, post-processed: Unknown size, total: 42.92 MiB) to /root/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f7928c1dc448fca7f08c036b0d763e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset opus_books downloaded and prepared to /root/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a65ac056d3047fe81f2ce263f5a72d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/102 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f64324018a44c00b7105c937d8bfed8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad55a044c382463ab8848a78e2d1afc0"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Load metrics","metadata":{}},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"sacrebleu\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n\n    return preds, labels\n\ndef compute_metrics(preds, labels):\n#     preds, labels = eval_preds\n    preds = preds.detach().cpu()\n    labels = labels.detach().cpu()\n    decoded_preds = tokenizer_encoder.batch_decode(preds, skip_special_tokens=True)\n    \n#     labels = np.where(labels != -100, labels, tokenizer_decoder.pad_token_id)\n#     print(labels)\n    decoded_labels = tokenizer_encoder.batch_decode(labels, skip_special_tokens=True)\n    \n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n    \n    prediction_lens = [np.count_nonzero(pred != tokenizer_encoder.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2023-05-08T14:47:41.121787Z","iopub.execute_input":"2023-05-08T14:47:41.122462Z","iopub.status.idle":"2023-05-08T14:47:41.435547Z","shell.execute_reply.started":"2023-05-08T14:47:41.122421Z","shell.execute_reply":"2023-05-08T14:47:41.434412Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom transformers import get_scheduler\nfrom tqdm.auto import tqdm\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nnum_epochs = 1\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T12:48:10.953038Z","iopub.execute_input":"2023-05-08T12:48:10.954534Z","iopub.status.idle":"2023-05-08T12:48:10.967669Z","shell.execute_reply.started":"2023-05-08T12:48:10.954489Z","shell.execute_reply":"2023-05-08T12:48:10.966731Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"progress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T13:45:16.896504Z","iopub.execute_input":"2023-05-08T13:45:16.897747Z","iopub.status.idle":"2023-05-08T14:40:57.235127Z","shell.execute_reply.started":"2023-05-08T13:45:16.897696Z","shell.execute_reply":"2023-05-08T14:40:57.233856Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/12709 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"184d1c7e7e5749ab92408b19097f578e"}},"metadata":{}}]},{"cell_type":"code","source":" trainer.save_model(\"/kaggle/working/saves/\")","metadata":{"execution":{"iopub.status.busy":"2023-05-08T13:45:13.567998Z","iopub.execute_input":"2023-05-08T13:45:13.568823Z","iopub.status.idle":"2023-05-08T13:45:14.077776Z","shell.execute_reply.started":"2023-05-08T13:45:13.568777Z","shell.execute_reply":"2023-05-08T13:45:14.076103Z"},"trusted":true},"execution_count":12,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/351089611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/saves/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"for i, batch in enumerate(eval_dataloader):\n    batch = {k: v.to(device) for k, v in batch.items()}\n\n    inputs = tokenizer_encoder.batch_decode(batch['input_ids'])\n    truth = tokenizer_encoder.batch_decode(batch['labels'])\n    outputs = model.generate(batch['input_ids'], max_length=150)\n    \n#     print(compute_metrics(outputs, batch['labels']))\n    \n    dec_outputs = tokenizer_encoder.batch_decode(outputs)\n    print(f\"\"\"\n> {inputs[1]}\n* {truth[1]}\n< {dec_outputs[1]}\n    \"\"\")\n    if i == 5:\n        break\n\n# decoded_labels = tokenizer_decoder.batch_decode(labels, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T15:54:23.857353Z","iopub.execute_input":"2023-05-08T15:54:23.859506Z","iopub.status.idle":"2023-05-08T15:57:04.241659Z","shell.execute_reply.started":"2023-05-08T15:54:23.859463Z","shell.execute_reply":"2023-05-08T15:57:04.240362Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"\n> [CLS]\"You saw?\" he said.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n* [CLS]--Tu as vu? dit-il.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n< \u0002[PAD]-- Vous êtes prises, dit Arthos Conseigne a Marguteries.[SEP][PAD][PAD][PAD]I. Homme Cramille Harberte filiciles! Chandimesse!» suiture de th[SEP][PAD]I fffrexinesthemimp\u0002\n    \n\n> [CLS]Whenever he saw it he would growl and back at a rapid rate, with his tail shut down, and the moment it was put upon the stove [SEP]\n* [CLS]Des qu’il l’apercevait, il grondait et reculait vivement, la queue entre les pattes, et des qu’on la plaçait sur le réchaud, i[SEP]\n< \u0002[PAD]Il avait une homme qui devant le prendre, et il faut contenuire à la maison du moins d'un petit, sur l'extressance d’avoir au [SEP][PAD][PAD][PAD]-douc détausss ll\u0002\n    \n\n> [CLS]\"Admitted,\" replied Mr. Fogg, coldly.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n* [CLS]-- Avoués, répondit froidement Mr. Fogg.[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n< \u0002[PAD]-- Parfection, répondit Cyrus Smith Spilett, Fabrizzio Crimaled.[SEP][PAD][PAD][PAD]II Chappelied and the Duchesses. Compinge Clicalle what I [SEP]X Ithinofff s tores \u0002\n    \n\n> [CLS]\"How is your superb house in the Rue Saint−André des Arcs coming on?[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n* [CLS]– Où en est votre superbe maison de la rue Saint-André-des-Arcs?[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n< \u0002[PAD]-- Vous parlez donc, dit Artagnan, répondis-je ne pouvais de contressions en plusieurs.[SEP][PAD][PAD][PAD]Ils n'est pendaient le manier du Pa[SEP][PAD]IInofffinoridex sof\u0002\n    \n\n> [CLS]Well, you are fine, to be sure!\"[SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n* [CLS]Eh! comme vous voilà beau, pour sûr![SEP][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD][PAD]\n< \u0002[PAD][PAD][PAD]eure, monsieux, répondit Harbert, the Princesse with his said to had and beare for frigher all shement ting table as iffect [SEP][PAD][PAD]Ine t weres arifin\u0002\n    \n\n> [CLS]Phileas Fogg, having shut the door of his house at half-past eleven, and having put his right foot before his left five hundre[SEP]\n* [CLS]Phileas Fogg avait quitté sa maison de Saville-row à onze heures et demie, et, après avoir placé cinq cent soixante-quinze foi[SEP]\n< \u0002[PAD]Il avait été par une fois, et les conseillement de cette heure, il faut pour la plusie dans sous longtemps du bonher se rappel[SEP][PAD][PAD][PAD], aritareneroutai\u0002\n    \n","output_type":"stream"}]},{"cell_type":"code","source":"bleu_scores = []\nfor i, batch in enumerate(eval_dataloader):\n    if i%200 == 0:\n        print(i)\n    batch = {k: v.to(device) for k, v in batch.items()}\n    output = model.generate(batch['input_ids'], max_length=100)\n    \n    bleu_score = compute_metrics(output, batch['labels'])['bleu']\n    bleu_scores.append(bleu_score)","metadata":{"execution":{"iopub.status.busy":"2023-05-08T15:41:31.286684Z","iopub.execute_input":"2023-05-08T15:41:31.287052Z","iopub.status.idle":"2023-05-08T15:48:09.960026Z","shell.execute_reply.started":"2023-05-08T15:41:31.287020Z","shell.execute_reply":"2023-05-08T15:48:09.958266Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/669628555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bleu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1482\u001b[0m                 \u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_dict_in_generate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m             )\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2739\u001b[0m             )  # (batch_size * num_beams, vocab_size)\n\u001b[1;32m   2740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2741\u001b[0;31m             \u001b[0mnext_token_scores_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_token_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2742\u001b[0m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_token_scores_processed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbeam_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_ids, scores)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mnum_batch_hypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mcur_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         \u001b[0mbanned_batch_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_calc_banned_ngram_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batch_hypotheses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbanned_tokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbanned_batch_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m_calc_banned_ngram_tokens\u001b[0;34m(ngram_size, prev_input_ids, num_hypos, cur_len)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hypos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m     \u001b[0mgenerated_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_hypos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     banned_tokens = [\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/generation/logits_process.py\u001b[0m in \u001b[0;36m_get_ngrams\u001b[0;34m(ngram_size, prev_input_ids, num_hypos)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mgenerated_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hypos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hypos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mgen_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprev_input_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mgenerated_ngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_ngrams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/bleu_score200.npy', np.array(bleu_scores))","metadata":{"execution":{"iopub.status.busy":"2023-05-08T15:40:01.149650Z","iopub.execute_input":"2023-05-08T15:40:01.150338Z","iopub.status.idle":"2023-05-08T15:40:01.155951Z","shell.execute_reply.started":"2023-05-08T15:40:01.150296Z","shell.execute_reply":"2023-05-08T15:40:01.154650Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"## Multilanguage (French/Hindi/Czech) to English (on wmt14)","metadata":{}},{"cell_type":"markdown","source":"To exploit the fully potentiel of being vocabulary-free, we have to train on different languages at once. Thus we learn to translate from french, hindi or czech (which all have very different languages properties) to english.\n\nNote that it would have been interested to enable the translation to any language, but it would have demanded to use a specific token on input, and then to fine-tune CANINE.","metadata":{}},{"cell_type":"markdown","source":"### Model Configuration","metadata":{}},{"cell_type":"markdown","source":"Load model","metadata":{}},{"cell_type":"code","source":"from transformers import CanineTokenizer, CanineConfig, BartConfig, EncoderDecoderConfig, EncoderDecoderModel\n\n\ntokenizer_encoder = CanineTokenizer.from_pretrained(\"google/canine-c\")\n\nconfig_encoder = CanineConfig().from_pretrained(\"google/canine-c\")\nconfig_decoder = BartConfig().from_pretrained(\"facebook/bart-base\")\nconfig_decoder.vocab_size = tokenizer_encoder.vocab_size\nconfig_decoder.max_position_embeddings = 128\n\nconfig = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n\nmodel = EncoderDecoderModel(config).to(device)\nmodel.config.decoder_start_token_id = model.decoder.config.decoder_start_token_id\nmodel.config.pad_token_id = model.decoder.config.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.692919Z","iopub.status.idle":"2023-05-01T15:16:04.693915Z","shell.execute_reply.started":"2023-05-01T15:16:04.693604Z","shell.execute_reply":"2023-05-01T15:16:04.693635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Freeze encoder","metadata":{}},{"cell_type":"code","source":"for param in model.encoder.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.696146Z","iopub.status.idle":"2023-05-01T15:16:04.697339Z","shell.execute_reply.started":"2023-05-01T15:16:04.697042Z","shell.execute_reply":"2023-05-01T15:16:04.697081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data loading","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\ntarget_lang = \"en\"\n \nfren_ds = load_dataset(\"wmt14\", 'fr-en') # 40836715\nhien_ds = load_dataset(\"wmt14\", 'hi-en') # 32863\ncsen_ds = load_dataset(\"wmt14\", 'cs-en') # 953621","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.698831Z","iopub.status.idle":"2023-05-01T15:16:04.699750Z","shell.execute_reply.started":"2023-05-01T15:16:04.699474Z","shell.execute_reply":"2023-05-01T15:16:04.699503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make a dataset containing 30000 examples of each languages","metadata":{}},{"cell_type":"code","source":"sample_fren_ds_train = fren_ds[\"train\"].shuffle().select(range(10000))\nsample_hien_ds_train = hien_ds[\"train\"].shuffle().select(range(500))\nsample_csen_ds_train = csen_ds[\"train\"].shuffle().select(range(500))\n\n# Convert language key into 'src' to be able to merge datasets\nsample_fren_ds_train = sample_fren_ds_train.map(lambda examples: format_lg(examples, \"fr\"), batched=True)\nsample_hien_ds_train = sample_hien_ds_train.map(lambda examples: format_lg(examples, \"hi\"), batched=True)\nsample_csen_ds_train = sample_csen_ds_train.map(lambda examples: format_lg(examples, \"cs\"), batched=True)\n\nfull_ds_train = concatenate_datasets((sample_fren_ds_train, sample_hien_ds_train, sample_csen_ds_train))","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.701753Z","iopub.status.idle":"2023-05-01T15:16:04.702376Z","shell.execute_reply.started":"2023-05-01T15:16:04.702079Z","shell.execute_reply":"2023-05-01T15:16:04.702109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_fren_ds_valid = fren_ds[\"validation\"].shuffle().select(range(500))\nsample_hien_ds_valid = hien_ds[\"validation\"].shuffle().select(range(500))\nsample_csen_ds_valid = csen_ds[\"validation\"].shuffle().select(range(500))\n\n# Convert language key into 'src' to be able to merge datasets\nsample_fren_ds_valid = sample_fren_ds_valid.map(lambda examples: format_lg(examples, \"fr\"), batched=True)\nsample_hien_ds_valid = sample_hien_ds_valid.map(lambda examples: format_lg(examples, \"hi\"), batched=True)\nsample_csen_ds_valid = sample_csen_ds_valid.map(lambda examples: format_lg(examples, \"cs\"), batched=True)\n\nfull_ds_valid = concatenate_datasets((sample_fren_ds_valid, sample_hien_ds_valid, sample_csen_ds_valid))","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.704917Z","iopub.status.idle":"2023-05-01T15:16:04.705463Z","shell.execute_reply.started":"2023-05-01T15:16:04.705178Z","shell.execute_reply":"2023-05-01T15:16:04.705205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data Loader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef preprocess_function(examples):\n    inputs = [example[\"src\"] for example in examples[\"translation\"]]\n    targets = [example[\"en\"] for example in examples[\"translation\"]]\n    model_inputs = tokenizer_encoder(inputs, text_target=targets, max_length=128, truncation=True, padding=\"max_length\")\n    return model_inputs\n\ntokenized_ds_train = full_ds_train.map(preprocess_function, batched=True)\ntokenized_ds_train = tokenized_ds_train.remove_columns([\"translation\"])\ntokenized_ds_train.set_format(\"torch\")\n\ntokenized_ds_valid = full_ds_valid.map(preprocess_function, batched=True)\ntokenized_ds_valid = tokenized_ds_valid.remove_columns([\"translation\"])\ntokenized_ds_valid.set_format(\"torch\")\n\ntrain_dataloader = DataLoader(tokenized_ds_train, shuffle=True, batch_size=1)\nvalid_dataloader = DataLoader(tokenized_ds_valid, shuffle=True, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.707604Z","iopub.status.idle":"2023-05-01T15:16:04.708140Z","shell.execute_reply.started":"2023-05-01T15:16:04.707860Z","shell.execute_reply":"2023-05-01T15:16:04.707886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom transformers import get_scheduler\nfrom tqdm.auto import tqdm\n\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\nnum_epochs = 1\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.710301Z","iopub.status.idle":"2023-05-01T15:16:04.710820Z","shell.execute_reply.started":"2023-05-01T15:16:04.710552Z","shell.execute_reply":"2023-05-01T15:16:04.710578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"progress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.713003Z","iopub.status.idle":"2023-05-01T15:16:04.713635Z","shell.execute_reply.started":"2023-05-01T15:16:04.713335Z","shell.execute_reply":"2023-05-01T15:16:04.713366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.715767Z","iopub.status.idle":"2023-05-01T15:16:04.716282Z","shell.execute_reply.started":"2023-05-01T15:16:04.716004Z","shell.execute_reply":"2023-05-01T15:16:04.716029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    inputs = tokenizer_encoder.batch_decode(batch['input_ids'])\n    truth = tokenizer_encoder.batch_decode(batch['labels'])\n    outputs = model.generate(batch['input_ids'], max_length=100)\n    dec_outputs = tokenizer_encoder.batch_decode(outputs)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.718303Z","iopub.status.idle":"2023-05-01T15:16:04.718797Z","shell.execute_reply.started":"2023-05-01T15:16:04.718540Z","shell.execute_reply":"2023-05-01T15:16:04.718565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Only French to English","metadata":{}},{"cell_type":"markdown","source":"As the embedding is too expensive, we have to reduce its length, and then only use \"french\" unicode characters","metadata":{}},{"cell_type":"markdown","source":"### Model Configuration","metadata":{}},{"cell_type":"markdown","source":"Load the model","metadata":{}},{"cell_type":"code","source":"from transformers import CanineTokenizer, CanineConfig, BartConfig, EncoderDecoderConfig, EncoderDecoderModel\n\ntokenizer_encoder = CanineTokenizer.from_pretrained(\"google/canine-c\")\n\nconfig_encoder = CanineConfig().from_pretrained(\"google/canine-c\")\nconfig_decoder = BartConfig().from_pretrained(\"facebook/bart-base\")\nconfig_decoder.vocab_size = 2*tokenizer_encoder.eos_token_id + 1\nconfig_decoder.max_position_embeddings = 128\n\nconfig = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n\nmodel = EncoderDecoderModel(config).to(device)\nmodel.config.decoder_start_token_id = model.decoder.config.decoder_start_token_id\nmodel.config.pad_token_id = model.decoder.config.pad_token_id;","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.720790Z","iopub.status.idle":"2023-05-01T15:16:04.721410Z","shell.execute_reply.started":"2023-05-01T15:16:04.721075Z","shell.execute_reply":"2023-05-01T15:16:04.721106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Freeze encoder","metadata":{}},{"cell_type":"code","source":"for param in model.encoder.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.723566Z","iopub.status.idle":"2023-05-01T15:16:04.724143Z","shell.execute_reply.started":"2023-05-01T15:16:04.723838Z","shell.execute_reply":"2023-05-01T15:16:04.723867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loading","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom torch.utils.data import DataLoader\n\nfren_ds = load_dataset(\"wmt14\", 'de-en') # 40836715","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.726304Z","iopub.status.idle":"2023-05-01T15:16:04.726791Z","shell.execute_reply.started":"2023-05-01T15:16:04.726538Z","shell.execute_reply":"2023-05-01T15:16:04.726564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create Dataset","metadata":{}},{"cell_type":"code","source":"sample_ds_train = fren_ds[\"train\"].shuffle().select(range(10000))\nsample_ds_valid = fren_ds[\"validation\"].shuffle().select(range(500))","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.728837Z","iopub.status.idle":"2023-05-01T15:16:04.729356Z","shell.execute_reply.started":"2023-05-01T15:16:04.729083Z","shell.execute_reply":"2023-05-01T15:16:04.729109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create Dataloader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef preprocess_function(examples):\n    inputs = [example[\"de\"] for example in examples[\"translation\"]]\n    targets = [example[\"en\"] for example in examples[\"translation\"]]\n    model_inputs = tokenizer_encoder(inputs, text_target=targets, max_length=128, truncation=True, padding=\"max_length\")\n    return model_inputs\n\ntokenized_ds_train = sample_ds_train.map(preprocess_function, batched=True)\ntokenized_ds_train = tokenized_ds_train.remove_columns([\"translation\"])\ntokenized_ds_train.set_format(\"torch\")\n\ntokenized_ds_valid = sample_ds_valid.map(preprocess_function, batched=True)\ntokenized_ds_valid = tokenized_ds_valid.remove_columns([\"translation\"])\ntokenized_ds_valid.set_format(\"torch\")\n\ntrain_dataloader = DataLoader(tokenized_ds_train, shuffle=True, batch_size=1)\nvalid_dataloader = DataLoader(tokenized_ds_valid, shuffle=True, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.731544Z","iopub.status.idle":"2023-05-01T15:16:04.732087Z","shell.execute_reply.started":"2023-05-01T15:16:04.731798Z","shell.execute_reply":"2023-05-01T15:16:04.731824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom transformers import get_scheduler\nfrom tqdm.auto import tqdm\n\noptimizer = AdamW(model.parameters(), lr=1e-5)\n\nnum_epochs = 1\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.734322Z","iopub.status.idle":"2023-05-01T15:16:04.734843Z","shell.execute_reply.started":"2023-05-01T15:16:04.734588Z","shell.execute_reply":"2023-05-01T15:16:04.734615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"progress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.736862Z","iopub.status.idle":"2023-05-01T15:16:04.737384Z","shell.execute_reply.started":"2023-05-01T15:16:04.737110Z","shell.execute_reply":"2023-05-01T15:16:04.737136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.739441Z","iopub.status.idle":"2023-05-01T15:16:04.739930Z","shell.execute_reply.started":"2023-05-01T15:16:04.739678Z","shell.execute_reply":"2023-05-01T15:16:04.739703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    inputs = tokenizer_encoder.batch_decode(batch['input_ids'])\n    truth = tokenizer_encoder.batch_decode(batch['labels'])\n    outputs = model.generate(batch['input_ids'], max_length=100)\n    dec_outputs = tokenizer_encoder.batch_decode(outputs)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.741625Z","iopub.status.idle":"2023-05-01T15:16:04.742544Z","shell.execute_reply.started":"2023-05-01T15:16:04.742278Z","shell.execute_reply":"2023-05-01T15:16:04.742307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using T5","metadata":{}},{"cell_type":"code","source":"model2 = EncoderDecoderModel.from_pretrained('/kaggle/working/save').to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.744164Z","iopub.status.idle":"2023-05-01T15:16:04.745062Z","shell.execute_reply.started":"2023-05-01T15:16:04.744808Z","shell.execute_reply":"2023-05-01T15:16:04.744837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n#     print(batch)\n    inputs = tokenizer_encoder.batch_decode(batch['input_ids'])\n    truth = tokenizer_encoder.batch_decode(batch['labels'])\n    print(inputs[0:3])\n    print('----------')\n    print(truth[0:3])\n    print('----------')\n    outputs = model2.generate(batch['input_ids'], max_length=100)\n    print(outputs.shape)\n    dec_outputs = tokenizer_encoder.batch_decode(outputs)\n    print(dec_outputs[0:3])\n#     print('----------')\n    break\n\n# decoded_labels = tokenizer_decoder.batch_decode(labels, skip_special_tokens=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.746509Z","iopub.status.idle":"2023-05-01T15:16:04.747687Z","shell.execute_reply.started":"2023-05-01T15:16:04.747387Z","shell.execute_reply":"2023-05-01T15:16:04.747415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/save","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.748973Z","iopub.status.idle":"2023-05-01T15:16:04.749837Z","shell.execute_reply.started":"2023-05-01T15:16:04.749579Z","shell.execute_reply":"2023-05-01T15:16:04.749605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv /kaggle/working/*.json /kaggle/working/save","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.751285Z","iopub.status.idle":"2023-05-01T15:16:04.752139Z","shell.execute_reply.started":"2023-05-01T15:16:04.751877Z","shell.execute_reply":"2023-05-01T15:16:04.751904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Using Custom Decoder","metadata":{}},{"cell_type":"code","source":"from transformers import CanineTokenizer, CanineConfig, CanineModel\n\nconfig = CanineConfig()\ntokenizer = CanineTokenizer.from_pretrained(\"google/canine-c\")\nmodel = CanineModel.from_pretrained(\"google/canine-c\").to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.753885Z","iopub.status.idle":"2023-05-01T15:16:04.754994Z","shell.execute_reply.started":"2023-05-01T15:16:04.754717Z","shell.execute_reply":"2023-05-01T15:16:04.754751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LENGTH=500","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.756311Z","iopub.status.idle":"2023-05-01T15:16:04.757175Z","shell.execute_reply.started":"2023-05-01T15:16:04.756902Z","shell.execute_reply":"2023-05-01T15:16:04.756930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n\n    def forward(self, inputs, hidden):\n        print(inputs)\n        embedded = self.embedding(inputs).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        return output, hidden\n\n    def init_hidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.758656Z","iopub.status.idle":"2023-05-01T15:16:04.759524Z","shell.execute_reply.started":"2023-05-01T15:16:04.759260Z","shell.execute_reply":"2023-05-01T15:16:04.759288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout=0.1, max_length=MAX_LENGTH):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout = dropout\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, inputs, hidden, encoder_outputs):\n        print(inputs)\n        embedded = self.embedding(inputs).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def init_hidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.761192Z","iopub.status.idle":"2023-05-01T15:16:04.762089Z","shell.execute_reply.started":"2023-05-01T15:16:04.761823Z","shell.execute_reply":"2023-05-01T15:16:04.761851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenizer = CanineTokenizer.from_pretrained(\"google/canine-c\")\n\nfr_seqs = [\"Je suis un homme\"]\nen_seqs = [\"I am a man\"]\ninput_tensor = tokenizer(fr_seqs, padding=True, return_tensors='pt')\noutput_tensor = tokenizer(en_seqs, padding=True, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.763692Z","iopub.status.idle":"2023-05-01T15:16:04.764707Z","shell.execute_reply.started":"2023-05-01T15:16:04.764417Z","shell.execute_reply":"2023-05-01T15:16:04.764444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hidden_size = 256\nencoder = EncoderRNN(config.eos_token_id+1, hidden_size).to(device)\nattn_decoder = AttnDecoderRNN(hidden_size, config.eos_token_id+1, dropout=0.1).to(device)\n\nencoder_hidden = encoder.init_hidden()\nfor ei in range(input_tensor.input_ids.shape[1]):\n    encoder_output, encoder_hidden = encoder(input_tensor.input_ids[ei], encoder_hidden)\n    encoder_outputs[ei] = encoder_output[0, 0]","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.766181Z","iopub.status.idle":"2023-05-01T15:16:04.767053Z","shell.execute_reply.started":"2023-05-01T15:16:04.766800Z","shell.execute_reply":"2023-05-01T15:16:04.766828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teacher_forcing_ratio = 0.5\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.init_hidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() / target_length","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.768695Z","iopub.status.idle":"2023-05-01T15:16:04.769729Z","shell.execute_reply.started":"2023-05-01T15:16:04.769443Z","shell.execute_reply":"2023-05-01T15:16:04.769477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___________","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"config = CanineConfig.from_pretrained(\"google/canine-c\")\ncanine_c = CanineModel.from_pretrained('google/canine-c')\ntokenizer = CanineTokenizer.from_pretrained('google/canine-c')\n\ntoklen = len(tokenizer)\nhid = config.hidden_size","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.771026Z","iopub.status.idle":"2023-05-01T15:16:04.772152Z","shell.execute_reply.started":"2023-05-01T15:16:04.771855Z","shell.execute_reply":"2023-05-01T15:16:04.771885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"________","metadata":{}},{"cell_type":"markdown","source":"## CANINE","metadata":{}},{"cell_type":"code","source":"from transformers import CanineTokenizer, CanineModel\n\nmodel = CanineModel.from_pretrained(\"google/canine-c\")\ntokenizer = CanineTokenizer.from_pretrained(\"google/canine-c\")\n\ninputs = [\"Life is like a box of chocolates.\", \"You never know what you gonna get.\", \"I juste want to try something with a long sentence you know.\"]\nencoding = tokenizer(inputs, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n\noutputs = model(**encoding)  # forward pass\npooled_output = outputs.pooler_output \nsequence_output = outputs.last_hidden_state  # (batch_size, seq_length, hidden_dim)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.774430Z","iopub.status.idle":"2023-05-01T15:16:04.774947Z","shell.execute_reply.started":"2023-05-01T15:16:04.774694Z","shell.execute_reply":"2023-05-01T15:16:04.774720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.last_hidden_state.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.777176Z","iopub.status.idle":"2023-05-01T15:16:04.777701Z","shell.execute_reply.started":"2023-05-01T15:16:04.777449Z","shell.execute_reply":"2023-05-01T15:16:04.777474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# fren_ds = load_dataset(\"wmt14\", 'fr-en')\nbooks = load_dataset(\";:<<<<<\", \"en-fr\")\nbooks = books[\"train\"].train_test_split(test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2023-05-01T15:16:04.779766Z","iopub.status.idle":"2023-05-01T15:16:04.780355Z","shell.execute_reply.started":"2023-05-01T15:16:04.780013Z","shell.execute_reply":"2023-05-01T15:16:04.780040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_________________","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}